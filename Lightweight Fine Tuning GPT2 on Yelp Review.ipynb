{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "f35354cd",
      "cell_type": "markdown",
      "source": "# Lightweight Fine-Tuning Project",
      "metadata": {}
    },
    {
      "id": "560fb3ff",
      "cell_type": "markdown",
      "source": "TODO: In this cell, describe your choices for each of the following\n\n* PEFT technique: LoRA\n* Model: GPT-2\n* Evaluation approach: Trainer.evaluate\n* Fine-tuning dataset:  Yelp/yelp_review_full",
      "metadata": {}
    },
    {
      "id": "de8d76bb",
      "cell_type": "markdown",
      "source": "## Loading and Evaluating a Foundation Model\n\nTODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset.",
      "metadata": {}
    },
    {
      "id": "f551c63a",
      "cell_type": "code",
      "source": "# load yelp review dataset.\n\n!pip install --upgrade datasets==3.2.0 huggingface-hub==0.27.1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Defaulting to user installation because normal site-packages is not writeable\n,Requirement already satisfied: datasets==3.2.0 in /home/student/.local/lib/python3.10/site-packages (3.2.0)\n,Requirement already satisfied: huggingface-hub==0.27.1 in /home/student/.local/lib/python3.10/site-packages (0.27.1)\n,Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets==3.2.0) (0.70.16)\n,Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==3.2.0) (3.9.3)\n,Requirement already satisfied: tqdm>=4.66.3 in /home/student/.local/lib/python3.10/site-packages (from datasets==3.2.0) (4.67.1)\n,Requirement already satisfied: filelock in /home/student/.local/lib/python3.10/site-packages (from datasets==3.2.0) (3.13.1)\n,Requirement already satisfied: numpy>=1.17 in /home/student/.local/lib/python3.10/site-packages (from datasets==3.2.0) (1.26.4)\n,Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==3.2.0) (3.4.1)\n,Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==3.2.0) (0.3.8)\n,Requirement already satisfied: requests>=2.32.2 in /home/student/.local/lib/python3.10/site-packages (from datasets==3.2.0) (2.32.3)\n,Requirement already satisfied: packaging in /home/student/.local/lib/python3.10/site-packages (from datasets==3.2.0) (24.0)\n,Requirement already satisfied: pandas in /home/student/.local/lib/python3.10/site-packages (from datasets==3.2.0) (2.2.1)\n,Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /home/student/.local/lib/python3.10/site-packages (from datasets==3.2.0) (2024.2.0)\n,Requirement already satisfied: pyyaml>=5.1 in /home/student/.local/lib/python3.10/site-packages (from datasets==3.2.0) (6.0.1)\n,Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==3.2.0) (15.0.1)\n,Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/student/.local/lib/python3.10/site-packages (from huggingface-hub==0.27.1) (4.10.0)\n,Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.2.0) (6.0.5)\n,Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.2.0) (1.3.1)\n,Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.2.0) (1.9.4)\n,Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.2.0) (1.4.1)\n,Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.2.0) (4.0.3)\n,Requirement already satisfied: attrs>=17.3.0 in /home/student/.local/lib/python3.10/site-packages (from aiohttp->datasets==3.2.0) (23.2.0)\n,Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets==3.2.0) (2.2.1)\n,Requirement already satisfied: certifi>=2017.4.17 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets==3.2.0) (2024.2.2)\n,Requirement already satisfied: charset-normalizer<4,>=2 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets==3.2.0) (3.3.2)\n,Requirement already satisfied: idna<4,>=2.5 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets==3.2.0) (3.6)\n,Requirement already satisfied: pytz>=2020.1 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets==3.2.0) (2024.1)\n,Requirement already satisfied: tzdata>=2022.7 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets==3.2.0) (2024.1)\n,Requirement already satisfied: python-dateutil>=2.8.2 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets==3.2.0) (2.9.0.post0)\n,Requirement already satisfied: six>=1.5 in /home/student/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==3.2.0) (1.16.0)\n"
        }
      ],
      "execution_count": 1
    },
    {
      "id": "68f9e5b8",
      "cell_type": "code",
      "source": "# Split data into training and testing set.\n\nfrom datasets import load_dataset\n\ntrain_ds, test_ds = load_dataset(\"yelp/yelp_review_full\", split=['train', 'test'])\n\n\nfor entry in train_ds.select(range(3)):\n    label = entry[\"label\"]\n    text = entry[\"text\"]\n    print(f\"label={label}, text={text}\")\n    \nprint(\"\\n\")\n\nfor entry in test_ds.select(range(3)):\n    label = entry[\"label\"]\n    text = entry[\"text\"]\n    print(f\"label={label}, text={text}\")",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "label=4, text=dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\n,label=1, text=Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\n,label=3, text=Been going to Dr. Goldberg for over 10 years. I think I was one of his 1st patients when he started at MHMG. He's been great over the years and is really all about the big picture. It is because of him, not my now former gyn Dr. Markoff, that I found out I have fibroids. He explores all options with you and is very patient and understanding. He doesn't judge and asks all the right questions. Very thorough and wants to be kept in the loop on every aspect of your medical health and your life.\n,\n,\n,label=0, text=I got 'new' tires from them and within two weeks got a flat. I took my car to a local mechanic to see if i could get the hole patched, but they said the reason I had a flat was because the previous patch had blown - WAIT, WHAT? I just got the tire and never needed to have it patched? This was supposed to be a new tire. \\nI took the tire over to Flynn's and they told me that someone punctured my tire, then tried to patch it. So there are resentful tire slashers? I find that very unlikely. After arguing with the guy and telling him that his logic was far fetched he said he'd give me a new tire \\\"this time\\\". \\nI will never go back to Flynn's b/c of the way this guy treated me and the simple fact that they gave me a used tire!\n,label=0, text=Don't waste your time.  We had two different people come to our house to give us estimates for a deck (one of them the OWNER).  Both times, we never heard from them.  Not a call, not the estimate, nothing.\n,label=0, text=All I can say is the worst! We were the only 2 people in the place for lunch, the place was freezing and loaded with kids toys! 2 bicycles, a scooter, and an electronic keyboard graced the dining room. A fish tank with filthy, slimy fingerprints smeared all over it is there for your enjoyment.\\n\\nOur food came... no water to drink, no tea, medium temperature food. Of course its cold, just like the room, I never took my jacket off! The plates are too small, you food spills over onto some semi-clean tables as you sit in your completely worn out booth seat. The fried noodles were out of a box and nasty, the shrimp was mushy, the fried rice was bright yellow.\\n\\nWe asked for water, they brought us 1 in a SOLO cup for 2 people. I asked for hot tea, they said 10 minutes. What Chinese restaurant does not have hot tea available upon request?\\n\\nOver all.... my first and last visit to this place. The only good point was that it was cheap, and deservingly so.\n"
        }
      ],
      "execution_count": 2
    },
    {
      "id": "4935cb4d",
      "cell_type": "code",
      "source": "# Tokenize the review text with GPT2 tokenizer.\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.unk_token\n\nsmall_train_ds = train_ds.shuffle(seed=42).select(range(1000))\nsmall_test_ds = test_ds.shuffle(seed=42).select(range(100))\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"],padding=\"max_length\", truncation=True)\n\ntokenized_train_ds = small_train_ds.map(\n      tokenize_function , batched=True\n    )\ntokenized_test_ds = small_test_ds.map(\n        tokenize_function, batched=True\n    )",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ec3defda34d4f008a6d192126318fe9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3236bea38848465e8e8d184b35f72064",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "execution_count": 4
    },
    {
      "id": "f28c4a78",
      "cell_type": "code",
      "source": "from transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=5)\nmodel.config.pad_token_id = model.config.eos_token_id\n\n\nprint(model)\n",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n,You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "GPT2ForSequenceClassification(\n,  (transformer): GPT2Model(\n,    (wte): Embedding(50257, 768)\n,    (wpe): Embedding(1024, 768)\n,    (drop): Dropout(p=0.1, inplace=False)\n,    (h): ModuleList(\n,      (0-11): 12 x GPT2Block(\n,        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n,        (attn): GPT2Attention(\n,          (c_attn): Conv1D()\n,          (c_proj): Conv1D()\n,          (attn_dropout): Dropout(p=0.1, inplace=False)\n,          (resid_dropout): Dropout(p=0.1, inplace=False)\n,        )\n,        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n,        (mlp): GPT2MLP(\n,          (c_fc): Conv1D()\n,          (c_proj): Conv1D()\n,          (act): NewGELUActivation()\n,          (dropout): Dropout(p=0.1, inplace=False)\n,        )\n,      )\n,    )\n,    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n,  )\n,  (score): Linear(in_features=768, out_features=5, bias=False)\n,)\n"
        }
      ],
      "execution_count": 24
    },
    {
      "id": "019b9f55",
      "cell_type": "code",
      "source": "import numpy as np\nfrom transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\"test_trainer\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return {\"accuracy\": (predictions == labels).mean()}",
      "metadata": {},
      "outputs": [],
      "execution_count": 10
    },
    {
      "id": "00b0cef3",
      "cell_type": "code",
      "source": "trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_ds,\n    eval_dataset=tokenized_test_ds,\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n)\n\ntrainer.evaluate()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4d52a229",
      "cell_type": "markdown",
      "source": "## Performing Parameter-Efficient Fine-Tuning\n\nTODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights.",
      "metadata": {}
    },
    {
      "id": "5775fadf",
      "cell_type": "code",
      "source": "# Load model and lora-config\n\nfrom peft import LoraConfig\nconfig = LoraConfig(\n    task_type=\"SEQ_CLS\",\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.01,\n    modules_to_save = [\"score\"]\n    #lora_bias\n)\n\nfrom peft import get_peft_model\nlora_model = get_peft_model(model, config)",
      "metadata": {},
      "outputs": [],
      "execution_count": 25
    },
    {
      "id": "894046c0",
      "cell_type": "code",
      "source": "print(lora_model)\nlora_model.print_trainable_parameters()",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "PeftModelForSequenceClassification(\n,  (base_model): LoraModel(\n,    (model): GPT2ForSequenceClassification(\n,      (transformer): GPT2Model(\n,        (wte): Embedding(50257, 768)\n,        (wpe): Embedding(1024, 768)\n,        (drop): Dropout(p=0.1, inplace=False)\n,        (h): ModuleList(\n,          (0-11): 12 x GPT2Block(\n,            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n,            (attn): GPT2Attention(\n,              (c_attn): Linear(\n,                in_features=768, out_features=2304, bias=True\n,                (lora_dropout): ModuleDict(\n,                  (default): Dropout(p=0.01, inplace=False)\n,                )\n,                (lora_A): ModuleDict(\n,                  (default): Linear(in_features=768, out_features=8, bias=False)\n,                )\n,                (lora_B): ModuleDict(\n,                  (default): Linear(in_features=8, out_features=2304, bias=False)\n,                )\n,                (lora_embedding_A): ParameterDict()\n,                (lora_embedding_B): ParameterDict()\n,              )\n,              (c_proj): Conv1D()\n,              (attn_dropout): Dropout(p=0.1, inplace=False)\n,              (resid_dropout): Dropout(p=0.1, inplace=False)\n,            )\n,            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n,            (mlp): GPT2MLP(\n,              (c_fc): Conv1D()\n,              (c_proj): Conv1D()\n,              (act): NewGELUActivation()\n,              (dropout): Dropout(p=0.1, inplace=False)\n,            )\n,          )\n,        )\n,        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n,      )\n,      (score): ModulesToSaveWrapper(\n,        (original_module): Linear(in_features=768, out_features=5, bias=False)\n,        (modules_to_save): ModuleDict(\n,          (default): Linear(in_features=768, out_features=5, bias=False)\n,        )\n,      )\n,    )\n,  )\n,)\n,trainable params: 302,592 || all params: 124,742,400 || trainable%: 0.24257349545944282\n"
        }
      ],
      "execution_count": 26
    },
    {
      "id": "9e4e54d4",
      "cell_type": "code",
      "source": "train_lora = tokenized_train_ds.rename_column('label', 'labels').remove_columns(\"text\")\ntest_lora = tokenized_test_ds.rename_column('label', 'labels').remove_columns(\"text\")",
      "metadata": {},
      "outputs": [],
      "execution_count": 13
    },
    {
      "id": "c4d4c908",
      "cell_type": "code",
      "source": "# Start the training process\n\nfrom transformers import DataCollatorWithPadding\n\n\nlora_trainer = Trainer(\n    model=lora_model,\n    args=TrainingArguments(\n        output_dir=\"./data/sentiment_analysis\",\n        learning_rate=2e-3,\n        # Reduce the batch size if you don't have enough memory\n        per_device_train_batch_size=4,\n        per_device_eval_batch_size=4,\n        num_train_epochs=4,\n        weight_decay=0.01,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        remove_unused_columns=False,\n        label_names=[\"labels\"],\n        save_safetensors=False,\n    ),\n    train_dataset=train_lora,\n    eval_dataset=test_lora,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n    compute_metrics=compute_metrics,\n)\n\nlora_trainer.train();\nmetrics = lora_trainer.evaluate();\nprint(metrics)\n\n\n\n",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 14:58, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.301690</td>\n",
              "      <td>0.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.542700</td>\n",
              "      <td>1.408385</td>\n",
              "      <td>0.460000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.542700</td>\n",
              "      <td>1.187754</td>\n",
              "      <td>0.530000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.991800</td>\n",
              "      <td>1.211912</td>\n",
              "      <td>0.540000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "Checkpoint destination directory ./data/sentiment_analysis/checkpoint-250 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n,Checkpoint destination directory ./data/sentiment_analysis/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n,Checkpoint destination directory ./data/sentiment_analysis/checkpoint-750 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n,Checkpoint destination directory ./data/sentiment_analysis/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 00:09]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "{'eval_loss': 1.1877543926239014, 'eval_accuracy': 0.53, 'eval_runtime': 9.4965, 'eval_samples_per_second': 10.53, 'eval_steps_per_second': 2.633, 'epoch': 4.0}\n"
        }
      ],
      "execution_count": 27
    },
    {
      "id": "6c6408f2",
      "cell_type": "code",
      "source": "lora_model.save_pretrained(\"gpt-lora-yelp-final2\")\n\n",
      "metadata": {},
      "outputs": [],
      "execution_count": 28
    },
    {
      "id": "fa7fe003",
      "cell_type": "code",
      "source": "print(lora_model)",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "PeftModelForSequenceClassification(\n,  (base_model): LoraModel(\n,    (model): GPT2ForSequenceClassification(\n,      (transformer): GPT2Model(\n,        (wte): Embedding(50257, 768)\n,        (wpe): Embedding(1024, 768)\n,        (drop): Dropout(p=0.1, inplace=False)\n,        (h): ModuleList(\n,          (0-11): 12 x GPT2Block(\n,            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n,            (attn): GPT2Attention(\n,              (c_attn): Linear(\n,                in_features=768, out_features=2304, bias=True\n,                (lora_dropout): ModuleDict(\n,                  (default): Dropout(p=0.01, inplace=False)\n,                )\n,                (lora_A): ModuleDict(\n,                  (default): Linear(in_features=768, out_features=8, bias=False)\n,                )\n,                (lora_B): ModuleDict(\n,                  (default): Linear(in_features=8, out_features=2304, bias=False)\n,                )\n,                (lora_embedding_A): ParameterDict()\n,                (lora_embedding_B): ParameterDict()\n,              )\n,              (c_proj): Conv1D()\n,              (attn_dropout): Dropout(p=0.1, inplace=False)\n,              (resid_dropout): Dropout(p=0.1, inplace=False)\n,            )\n,            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n,            (mlp): GPT2MLP(\n,              (c_fc): Conv1D()\n,              (c_proj): Conv1D()\n,              (act): NewGELUActivation()\n,              (dropout): Dropout(p=0.1, inplace=False)\n,            )\n,          )\n,        )\n,        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n,      )\n,      (score): ModulesToSaveWrapper(\n,        (original_module): Linear(in_features=768, out_features=5, bias=False)\n,        (modules_to_save): ModuleDict(\n,          (default): Linear(in_features=768, out_features=5, bias=False)\n,        )\n,      )\n,    )\n,  )\n,)\n"
        }
      ],
      "execution_count": 29
    },
    {
      "id": "615b12c6",
      "cell_type": "markdown",
      "source": "## Performing Inference with a PEFT Model\n\nTODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning.",
      "metadata": {}
    },
    {
      "id": "863ec66e",
      "cell_type": "code",
      "source": "from peft import AutoPeftModelForSequenceClassification\nlora_model_loaded = AutoPeftModelForSequenceClassification.from_pretrained(\"gpt-lora-yelp-final2\", num_labels=5)",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n,You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        }
      ],
      "execution_count": 31
    },
    {
      "id": "03124cd9",
      "cell_type": "code",
      "source": "print(lora_model_loaded)",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "PeftModelForSequenceClassification(\n,  (base_model): LoraModel(\n,    (model): GPT2ForSequenceClassification(\n,      (transformer): GPT2Model(\n,        (wte): Embedding(50257, 768)\n,        (wpe): Embedding(1024, 768)\n,        (drop): Dropout(p=0.1, inplace=False)\n,        (h): ModuleList(\n,          (0-11): 12 x GPT2Block(\n,            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n,            (attn): GPT2Attention(\n,              (c_attn): Linear(\n,                in_features=768, out_features=2304, bias=True\n,                (lora_dropout): ModuleDict(\n,                  (default): Dropout(p=0.01, inplace=False)\n,                )\n,                (lora_A): ModuleDict(\n,                  (default): Linear(in_features=768, out_features=8, bias=False)\n,                )\n,                (lora_B): ModuleDict(\n,                  (default): Linear(in_features=8, out_features=2304, bias=False)\n,                )\n,                (lora_embedding_A): ParameterDict()\n,                (lora_embedding_B): ParameterDict()\n,              )\n,              (c_proj): Conv1D()\n,              (attn_dropout): Dropout(p=0.1, inplace=False)\n,              (resid_dropout): Dropout(p=0.1, inplace=False)\n,            )\n,            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n,            (mlp): GPT2MLP(\n,              (c_fc): Conv1D()\n,              (c_proj): Conv1D()\n,              (act): NewGELUActivation()\n,              (dropout): Dropout(p=0.1, inplace=False)\n,            )\n,          )\n,        )\n,        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n,      )\n,      (score): ModulesToSaveWrapper(\n,        (original_module): Linear(in_features=768, out_features=5, bias=False)\n,        (modules_to_save): ModuleDict(\n,          (default): Linear(in_features=768, out_features=5, bias=False)\n,        )\n,      )\n,    )\n,  )\n,)\n"
        }
      ],
      "execution_count": 32
    },
    {
      "id": "bc3a8147",
      "cell_type": "code",
      "source": "lora_model_loaded.config.pad_token_id = lora_model_loaded.config.eos_token_id\n\ntrainer = Trainer(\n    model=lora_model_loaded,\n    args=TrainingArguments(\n        output_dir=\"./data/sentiment_analysis_evaluate\",\n        label_names=[\"labels\"],\n    ),\n    train_dataset=train_lora,\n    eval_dataset=test_lora,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n    compute_metrics=compute_metrics,\n)\n\nprint(trainer.evaluate())\n\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=5)\nmodel.config.pad_token_id = model.config.eos_token_id\n\ntrainer = Trainer(\n    model=model,\n    args=TrainingArguments(\n        output_dir=\"./data/sentiment_analysis_evaluate\",\n        label_names=[\"labels\"],\n    ),\n    train_dataset=train_lora,\n    eval_dataset=test_lora,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n    compute_metrics=compute_metrics,\n)\n\nprint(trainer.evaluate())",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:08]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "{'eval_loss': 1.1877543926239014, 'eval_accuracy': 0.53, 'eval_runtime': 9.4002, 'eval_samples_per_second': 10.638, 'eval_steps_per_second': 1.383}\n"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n,You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:08]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "{'eval_loss': 6.752526760101318, 'eval_accuracy': 0.21, 'eval_runtime': 9.4509, 'eval_samples_per_second': 10.581, 'eval_steps_per_second': 1.376}\n"
        }
      ],
      "execution_count": 33
    },
    {
      "id": "bc96905a",
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "866ab28c",
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f9a32e4e",
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    }
  ]
}